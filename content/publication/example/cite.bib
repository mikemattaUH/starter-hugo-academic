
@article{vernice_online_2019,
	title = {An {Online} {Tool} to {Assess} {Sentence} {Comprehension} in {Teenagers} at {Risk} for {School} {Exclusion}: {Evidence} {From} {L2} {Italian} {Students}},
	volume = {10},
	issn = {1664-1078},
	shorttitle = {An {Online} {Tool} to {Assess} {Sentence} {Comprehension} in {Teenagers} at {Risk} for {School} {Exclusion}},
	url = {https://www.frontiersin.org/article/10.3389/fpsyg.2019.02417/full},
	doi = {10.3389/fpsyg.2019.02417},
	abstract = {This study presents a web-based sentence comprehension test aimed at identifying high school students who are at risk for a language delay. By assessing linguistic skills on a sample of high school students with Italian as an L2 and their monolingual peers, attending a vocational school, we were able to identify a subgroup of L2 students with consistent difﬁculties in sentence comprehension, though their reading skills were within the average range. The same subgroup revealed to experience a lack of support within the school context, suggesting that poor L2 skills might be a critical variable to consider in order to identify students at risk for school exclusion. Regarding the test, accuracy to the on-line sentence comprehension task was signiﬁcantly predicted by reading abilities and vocabulary skills, thus indicating that this test might represent a rapid but efﬁcient way to assess linguistic abilities at school. We recommend that establishing a valid and practical procedure for the evaluation of linguistic skills in bilingual students who struggle with their L2 is the ﬁrst step toward promoting social inclusion in the multilingual classroom, in order to increase their ability to actively participate in school and social activities.},
	language = {en},
	urldate = {2022-08-27},
	journal = {Frontiers in Psychology},
	author = {Vernice, Mirta and Matta, Michael and Tironi, Marta and Caccia, Martina and Lombardi, Elisabetta and Guasti, Maria Teresa and Sarti, Daniela and Lang, Margherita},
	month = nov,
	year = {2019},
	pages = {2417},
	file = {Vernice et al. - 2019 - An Online Tool to Assess Sentence Comprehension in.pdf:C\:\\Users\\Michael\\Zotero\\storage\\UZFS57IM\\Vernice et al. - 2019 - An Online Tool to Assess Sentence Comprehension in.pdf:application/pdf},
}

@article{beidler_diagnosed_2022,
	title = {Diagnosed and {Nondisclosed} {Sport}-{Related} {Concussion}: {An} {Exploratory} {Comparison} {Study} by {ADHD} {Status} in {Collegiate} {Athletes}},
	volume = {26},
	issn = {1087-0547, 1557-1246},
	shorttitle = {Diagnosed and {Nondisclosed} {Sport}-{Related} {Concussion}},
	url = {http://journals.sagepub.com/doi/10.1177/10870547211015432},
	doi = {10.1177/10870547211015432},
	abstract = {Objective:
              To determine if ADHD status in collegiate athletes was associated with differences in the number of diagnosed and nondisclosed sport-related concussions, and reasons why suspected concussive injuries were not reported.
            
            
              Method:
              A cross-sectional survey was completed by 858 collegiate athletes (65 with ADHD; 793 without ADHD). The survey included self-report items regarding ADHD status and histories of diagnosed and nondisclosed sport-related concussions. Groups were matched for gender, age, and sport with a ratio of one participant with ADHD to four without ADHD.
            
            
              Results:
              Collegiate athletes with ADHD reported significantly more diagnosed sport-related concussions and were more likely to have a history of diagnosis compared to those without ADHD. There were no significant differences between groups regarding sport-related concussion nondisclosure history.
            
            
              Conclusion:
              Collegiate athletes with ADHD have an increased prevalence rate of diagnosed sport-related concussions compared to those without ADHD; however, this developmental disorder may not influence injury nondisclosure decisions.},
	language = {en},
	number = {4},
	urldate = {2022-08-27},
	journal = {Journal of Attention Disorders},
	author = {Beidler, Erica and Schmitt, Ara J and Matta, Michael and Griger, Cassondra},
	month = feb,
	year = {2022},
	pages = {606--615},
	file = {Beidler et al. - 2022 - Diagnosed and Nondisclosed Sport-Related Concussio.pdf:C\:\\Users\\Michael\\Zotero\\storage\\CE6S3FWX\\Beidler et al. - 2022 - Diagnosed and Nondisclosed Sport-Related Concussio.pdf:application/pdf},
}

@article{briesch_development_2022,
	title = {Development of {Brief} {Rating} {Scales} for {Progress} {Monitoring} {Internalizing} {Behavior}},
	volume = {30},
	issn = {1063-4266, 1538-4799},
	url = {http://journals.sagepub.com/doi/10.1177/10634266211039761},
	doi = {10.1177/10634266211039761},
	abstract = {Prevalence estimates suggest that up to 20\% of students in schools experience significant internalizing behaviors that impact behavioral, social, or academic functioning. School-based interventions have great potential to promote student mental health; however, validated and feasible brief assessments are needed to progress monitor students’ responses to these supports. The purpose of the current study was twofold: to (a) develop and validate teacher-completed brief rating scales for progress monitoring internalizing concerns in elementary-aged students and (b) determine the reliability of the resultant measures. First, item content was generated and subjected to evaluation by two panels of school-based consumers and researchers. Within the second phase of development, exploratory and confirmatory factor analyses were used to reduce the initial number of items and ensure that the items were indicators of one latent factor. Teachers in grades K–3 (N = 307) each completed ratings for one randomly selected student in their classroom. Results of factor analysis for each scale indicated one-factor solutions for the 4-item Anxious/Depressed (ω = .88) and 4-item Socially Withdrawn (ω = .87) scales.},
	language = {en},
	number = {3},
	urldate = {2022-08-27},
	journal = {Journal of Emotional and Behavioral Disorders},
	author = {Briesch, Amy M. and Donaldson, Aberdine R. and Matta, Michael and Volpe, Robert J. and Daniels, Brian and Owens, Julie Sarno},
	month = sep,
	year = {2022},
	pages = {199--209},
	file = {Briesch et al. - 2022 - Development of Brief Rating Scales for Progress Mo.pdf:C\:\\Users\\Michael\\Zotero\\storage\\73R49D33\\Briesch et al. - 2022 - Development of Brief Rating Scales for Progress Mo.pdf:application/pdf},
}

@article{keller-margulis_validity_2021,
	title = {Validity of automated text evaluation tools for written-expression curriculum-based measurement: a comparison study},
	volume = {34},
	issn = {0922-4777, 1573-0905},
	shorttitle = {Validity of automated text evaluation tools for written-expression curriculum-based measurement},
	url = {https://link.springer.com/10.1007/s11145-021-10153-6},
	doi = {10.1007/s11145-021-10153-6},
	abstract = {Existing approaches to measuring writing performance are insufficient in terms of both technical adequacy as well as feasibility for use as a screening measure. This study examined the validity and diagnostic accuracy of several approaches to automated text evaluation as well as written expression curriculum-based measurement (WE-CBM) to determine whether an automated approach improves technical adequacy. A sample of 140 fourth grade students generated writing samples that were then scored using traditional and automated approaches and examined in relation to the statewide measure of writing performance. Results indicated that the validity and diagnostic accuracy for the best performing WE-CBM metric, correct minus incorrect word sequences, and the automated approaches to scoring were comparable, with automated approaches offering potentially improved feasibility for use in screening. Averaging scores across three time points was necessary, however, in order to achieve improved validity and adequate levels of diagnostic accuracy across the scoring approaches. Limitations, implications, and directions for future research regarding the use of automated scoring approaches for screening are discussed.},
	language = {en},
	number = {10},
	urldate = {2022-08-27},
	journal = {Reading and Writing},
	author = {Keller-Margulis, Milena A. and Mercer, Sterett H. and Matta, Michael},
	month = dec,
	year = {2021},
	pages = {2461--2480},
	file = {Keller-Margulis et al. - 2021 - Validity of automated text evaluation tools for wr.pdf:C\:\\Users\\Michael\\Zotero\\storage\\BJCX5DNL\\Keller-Margulis et al. - 2021 - Validity of automated text evaluation tools for wr.pdf:application/pdf},
}

@article{landry_long-term_2022,
	title = {Long-{Term} {Validity} and {Diagnostic} {Accuracy} of {Kindergarten} {Acadience} {Reading} with {English} {Learners}},
	volume = {51},
	issn = {2372-966X},
	url = {https://www.tandfonline.com/doi/full/10.1080/2372966X.2021.1984170},
	doi = {10.1080/2372966X.2021.1984170},
	abstract = {Acadience Reading (AR) is a screener for early detection of reading problems in elementary students. Limited research exists, however, on its technical adequacy for evaluation of English Learners (ELs). In this study, we tested the long-term predictive validity and diagnostic accuracies of AR and examined the differences between native English-speaking and EL students. A sample of 305 students (94 ELs, 31\% of the sample) completed AR at three time points in kindergarten and the statewide reading test in third grade. Hierarchical regression models confirmed good long-term validity for AR with end of the school year scores outperforming the other two time-points and EL status along with ethnic background not playing a significant role. Findings also revealed that AR at the end of the school year provided moderate long-term diagnostic accuracy for students well below benchmark. Implications for research and practice as well as limitations of the study are discussed.},
	language = {en},
	number = {4},
	urldate = {2022-08-27},
	journal = {School Psychology Review},
	author = {Landry, Lindsey N. and Keller-Margulis, Milena and Matta, Michael and Kim, Hanjoe and Gonzalez, Jorge E. and Thomas Schanding, G.},
	month = jul,
	year = {2022},
	pages = {454--467},
	file = {Landry et al. - 2022 - Long-Term Validity and Diagnostic Accuracy of Kind.pdf:C\:\\Users\\Michael\\Zotero\\storage\\BQ3CGPKC\\Landry et al. - 2022 - Long-Term Validity and Diagnostic Accuracy of Kind.pdf:application/pdf},
}

@article{lang_cognitive_2019,
	title = {Cognitive {Profile} of {Intellectually} {Gifted} {Adults}: {Analyzing} the {Wechsler} {Adult} {Intelligence} {Scale}},
	volume = {26},
	issn = {1073-1911, 1552-3489},
	shorttitle = {Cognitive {Profile} of {Intellectually} {Gifted} {Adults}},
	url = {http://journals.sagepub.com/doi/10.1177/1073191117733547},
	doi = {10.1177/1073191117733547},
	abstract = {The Wechsler Adult Intelligence Scale (WAIS) has been used extensively to study intellectual abilities of special groups. Here, we report the results of an intellectually gifted group on the WAIS-IV. Gifted individuals are people who obtained scores equal to or greater than 2 standard deviations above the mean on an intelligence test. Hence, the current study aims first, to examine mean group performance data of gifted individuals on the WAIS-IV; second, to revalidate the pattern of performance identified in this special group in previous studies (i.e., verbal skills higher than all other abilities); third, to compare scatter measures across intellectual domains with a matched comparison group. A total of 130 gifted individuals (79 males) were administered the full battery and their performance was compared with a matched comparison group. Analyses revealed that gifted group displayed higher scores in all intellectual domains. Contrary to expectations, they showed the highest scores in perceptual reasoning tasks. A multivariate approach revealed that this ability was statistically different from all other domains within the gifted group. Moreover, gifted individuals showed higher discrepancies across intellectual domains than average-intelligence people. Findings have important practical implications to detect intellectual giftedness in adulthood.},
	language = {en},
	number = {5},
	urldate = {2022-08-27},
	journal = {Assessment},
	author = {Lang, Margherita and Matta, Michael and Parolin, Laura and Morrone, Cristina and Pezzuti, Lina},
	month = jul,
	year = {2019},
	pages = {929--943},
	file = {Lang et al. - 2019 - Cognitive Profile of Intellectually Gifted Adults.pdf:C\:\\Users\\Michael\\Zotero\\storage\\WCGXH3ZZ\\Lang et al. - 2019 - Cognitive Profile of Intellectually Gifted Adults.pdf:application/pdf},
}

@article{lang_valutazione_2018,
	title = {La valutazione delle abilita cognitive dei soci {Mensa} {Italia}-{The} {High} {I}.{Q}. {Society}: uno studio preliminare della plusdotazione con la {Wechsler} {Adult} {Intelligence} {Scale}-{IV}},
	issn = {0390-5349},
	shorttitle = {La valutazione delle abilita\&\#768; cognitive dei soci {Mensa} {Italia}-{The} {High} {I}.{Q}. {Society}},
	url = {https://doi.org/10.1421/90622},
	doi = {10.1421/90622},
	language = {it},
	number = {2},
	urldate = {2022-08-27},
	journal = {Giornale italiano di psicologia},
	author = {Lang, Margherita and Matta, Michael and Michelotti, Clara and Brusadelli, Emanuela and Volpe, Serena},
	year = {2018},
	pages = {385--406},
	file = {Margherita Lang et al. - 2018 - La valutazione delle abilita&#768\; cognitive dei s.pdf:C\:\\Users\\Michael\\Zotero\\storage\\IR6CV56L\\Margherita Lang et al. - 2018 - La valutazione delle abilita&#768\; cognitive dei s.pdf:application/pdf},
}

@article{matta_personality_2019,
	title = {Personality assessment of intellectually gifted adults: {A} dimensional trait approach},
	volume = {140},
	issn = {01918869},
	shorttitle = {Personality assessment of intellectually gifted adults},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0191886918302563},
	doi = {10.1016/j.paid.2018.05.009},
	abstract = {Personality is often assessed by measuring the severity of dimensional traits regardless of intellectual functioning. However, extreme deviations from average cognitive abilities aﬀect biological, psychological, and social characteristics, to the extent that they may lead to atypical trajectories of development. The present study examined subclinical personality traits of intellectually gifted adults who unlikely fulﬁll traditional criteria for psychopathological diagnosis.},
	language = {en},
	urldate = {2022-08-27},
	journal = {Personality and Individual Differences},
	author = {Matta, Michael and Gritti, Emanuela Saveria and Lang, Margherita},
	month = apr,
	year = {2019},
	pages = {21--26},
	file = {Matta et al. - 2019 - Personality assessment of intellectually gifted ad.pdf:C\:\\Users\\Michael\\Zotero\\storage\\HZH2CR3J\\Matta et al. - 2019 - Personality assessment of intellectually gifted ad.pdf:application/pdf},
}

@article{matta_five_2020,
	title = {Five direct behavior rating multi-item scales: {Sensitivity} to the effects of classroom interventions},
	volume = {81},
	issn = {00224405},
	shorttitle = {Five direct behavior rating multi-item scales},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S002244052030025X},
	doi = {10.1016/j.jsp.2020.05.002},
	abstract = {Direct Behavior Rating (DBR) is a tool designed for the assessment of behavioral changes over time. Unlike methods for summative evaluations, the development of progress monitoring tools requires evaluation of sensitivity to change. The present study aimed to evaluate this psychometric feature of five newly developed DBR Multi-Item Scales (DBR-MIS). Teachers identified students with behaviors interfering with their learning or the learning of others and implemented a Daily Report Card (DRC) intervention in the classroom settings for two months. The analyses were performed on 31 AB single case studies. Change metrics were calculated at an individual level by using Tau-UA vs. B + trend B and Hedges' g and at a scale-level by using Mixed Effect MetaAnalysis, Hierarchical Linear Models (HLMs), and Between-Case Standardized Mean Difference (BC-SMD). HLMs were estimated considering both fixed and random effects of intervention and linear trend within the intervention phase. The results supported sensitivity to change for three DBR-MIS (i.e., Academic Engagement, Organizational Skills, and Disruptive Behavior), and the relative magnitudes were consistent across the metrics. Sensitivity to change of DBR-MIS Interpersonal Skills received moderate support. Conversely, empirical evidence was not provided for sensitivity to change of DBR-MIS Oppositional Behavior. Particular emphasis was placed on the intervention trend in that responses to behavioral interventions might occur gradually or require consistency over time in order to be observed by raters. Implications for the use of the new DBR-MIS in the context of progress monitoring of social-emotional behaviors are discussed.},
	language = {en},
	urldate = {2022-08-27},
	journal = {Journal of School Psychology},
	author = {Matta, Michael and Volpe, Robert J. and Briesch, Amy M. and Owens, Julie Sarno},
	month = aug,
	year = {2020},
	pages = {28--46},
	file = {Matta et al. - 2020 - Five direct behavior rating multi-item scales Sen.pdf:C\:\\Users\\Michael\\Zotero\\storage\\ZLY2IP9C\\Matta et al. - 2020 - Five direct behavior rating multi-item scales Sen.pdf:application/pdf},
}

@article{matta_cost_2022,
	title = {Cost analysis and cost-effectiveness of hand-scored and automated approaches to writing screening},
	volume = {92},
	issn = {00224405},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0022440522000231},
	doi = {10.1016/j.jsp.2022.03.003},
	abstract = {Although researchers have investigated technical adequacy and usability of written-expression curriculum-based measures (WE-CBM), the economic implications of different scoring ap­ proaches have largely been ignored. The absence of such knowledge can undermine the effective allocation of resources and lead to the adoption of suboptimal measures for the identification of students at risk for poor writing outcomes. Therefore, we used the Ingredients Method to compare implementation costs and cost-effectiveness of hand-calculated and automated scoring ap­ proaches. Data analyses were conducted on secondary data from a study that evaluated predictive validity and diagnostic accuracy of quantitative approaches for scoring WE-CBM samples. Find­ ings showed that automated approaches offered more economic solutions than hand-calculated methods; for automated scores, the effects were stronger when the free writeAlizer R package was employed, whereas for hand-calculated scores, simpler WE-CBM metrics were less costly than more complex metrics. Sensitivity analyses confirmed the relative advantage of automated scores when the number of classrooms, students, and assessment occasions per school year increased; again, writeAlizer was less sensitive to the changes in the ingredients than the other approaches. Finally, the visualization of the cost-effectiveness ratio illustrated that writeAlizer offered the optimal balance between implementation costs and diagnostic accuracy, followed by complex hand-calculated metrics and a proprietary automated program. Implications for the use of handcalculated and automated scores for the universal screening of written expression with elemen­ tary students are discussed.},
	language = {en},
	urldate = {2022-08-27},
	journal = {Journal of School Psychology},
	author = {Matta, Michael and Keller-Margulis, Milena A. and Mercer, Sterett H.},
	month = jun,
	year = {2022},
	pages = {80--95},
	file = {Matta et al. - 2022 - Cost analysis and cost-effectiveness of hand-score.pdf:C\:\\Users\\Michael\\Zotero\\storage\\MQHSF6YM\\Matta et al. - 2022 - Cost analysis and cost-effectiveness of hand-score.pdf:application/pdf},
}

@article{matta_evaluating_2022,
	title = {Evaluating validity and bias for hand-calculated and automated written expression curriculum-based measurement scores},
	volume = {29},
	issn = {0969-594X, 1465-329X},
	url = {https://www.tandfonline.com/doi/full/10.1080/0969594X.2022.2043240},
	doi = {10.1080/0969594X.2022.2043240},
	abstract = {Written expression curriculum-based measurement (WE-CBM) is a formative assessment approach for screening and progress mon­ itoring. To extend evaluation of WE-CBM, we compared handcalculated and automated scoring approaches in relation to the number of screening samples needed per student for valid scores, the long-term predictive validity and diagnostic accuracy of scores, and predictive and diagnostic bias for underrepresented student groups. Second- to fifth-grade students (n = 609) completed five WE-CBM tasks during one academic year and a standardised writing test in fourth and seventh grade. Averaging WE-CBM scores across multiple samples improved validity. Complex hand-calculated metrics and automated tools outperformed simpler metrics for the long-term prediction of writing performance. No evidence of bias was observed between African American and Hispanic stu­ dents. The study will illustrate the absence of test bias as necessary condition for fair and equitable screening procedures and the importance of future research to include comparisons with majority groups.},
	language = {en},
	number = {2},
	urldate = {2022-08-27},
	journal = {Assessment in Education: Principles, Policy \& Practice},
	author = {Matta, Michael and Mercer, Sterett H. and Keller-Margulis, Milena A.},
	month = mar,
	year = {2022},
	pages = {200--218},
	file = {Matta et al. - 2022 - Evaluating validity and bias for hand-calculated a.pdf:C\:\\Users\\Michael\\Zotero\\storage\\WQWTRWPU\\Matta et al. - 2022 - Evaluating validity and bias for hand-calculated a.pdf:application/pdf},
}